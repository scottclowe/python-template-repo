###############################################################################
language: python

# Current requirements for Python 3.7 on Travis
sudo: required
dist: xenial

###############################################################################
# Cache data which has to be downloaded on every build.
# This is just for Travis and doesn't do anything on Shippable.
cache:
  directories:
    # Cache files downloaded by pip
    - $HOME/.cache/pip
    # Cache our miniconda download.
    - $HOME/Downloads

###############################################################################
python:
  # This is a flag for the built-in version of Python provided by the CI-server
  # provider, which we don't use in favour of conda. But we use this to pick
  # out which python version we install with conda, since it means the provider
  # gets appropriate metadata to keep things organised.
  - "2.7"
  - "3.5"
  - "3.6"
  - "3.7"

###############################################################################
env:
  matrix:
    # Run tests with newest and oldest dependencies
    - OLDEST_DEPS="false"
    - OLDEST_DEPS="true"

matrix:
  # Mark as failure as soon as a required job fails. Otherwise, mark as success
  # as soon the only remaining jobs are allowed to fail.
  fast_finish: true

###############################################################################
# Setup the environment before installing
before_install:
  # Set documentation building option to always be enabled
  # If you only want to do this some of the time, set it with env above.
  - BUILD_DOCS="true"
  # Remember the directory where our repository to test is located
  - REPOPATH="$(pwd)" && pwd
  # ---------------------------------------------------------------------------
  # Check which versions of numpy and scipy we are using, then remove these
  # lines from requirements.txt
  - if [ -f requirements.txt ]; then
      NUMPY_REQUIREMENT="$(grep '^numpy\([!<>=~ ]\|$\)' requirements.txt)";
      echo "NumPy requirement is '$NUMPY_REQUIREMENT'";
      SCIPY_REQUIREMENT="$(grep '^scipy\([!<>=~ ]\|$\)' requirements.txt)";
      echo "SciPy requirement is '$SCIPY_REQUIREMENT'";
      sed '/^\(num\|sci\)py\([!<>=~ ]\|$\)/d' requirements.txt >
        requirements.txt.tmp &&
        mv requirements.txt.tmp requirements.txt;
    fi;
  # ---------------------------------------------------------------------------
  # Update the package list
  - travis_retry sudo apt-get update
  # Install numpy/scipy dependencies with apt-get. We want ATLAS and LAPACK.
  - if [[ "$NUMPY_REQUIREMENT" != "" ]] || [[ "$SCIPY_REQUIREMENT" != "" ]]; then
      travis_retry sudo apt-get install -y libatlas-dev libatlas-base-dev;
    fi;
    if [[ "$SCIPY_REQUIREMENT" != "" ]]; then
      travis_retry sudo apt-get install -y liblapack-dev;
    fi;
  # ---------------------------------------------------------------------------
  # If we want to run the tests using the oldest set of dependencies we
  # support, modify any *requirements*.txt files every '>=' becomes '=='.
  - if [[ "$OLDEST_DEPS" == "true" ]]; then
      for FILE in *requirements*.txt; do
          sed -e 's/>=/~=/g' $FILE > $FILE.tmp && mv $FILE.tmp $FILE;
      done;
    fi;
  # ---------------------------------------------------------------------------
  # A minority of packages need to be installed through conda instead of pip
  # for a variety of reasons. PACKAGES_TO_CONDA is a space separated list of
  # package names which should be installed using conda if they are in the
  # requirements. Packages not in this list will be installed through pip.
  - PACKAGES_TO_CONDA="Cython h5py";
  # Now we automatically search for these package settings in requirements.txt
  - OTHER_CONDA_REQUIREMENTS="";
    if [ -f requirements.txt ]; then
        while read -r PKG_NAME; do
            THIS_REQUIREMENT="$(grep "^$PKG_NAME\([!<>=~ ]\|$\)" requirements.txt)";
            OTHER_CONDA_REQUIREMENTS+=" $THIS_REQUIREMENT";
            sed -i "/^$PKG_NAME\([!<>=~ ]\|$\)/d" requirements.txt;
        done < <(echo "$PACKAGES_TO_CONDA" | tr ' ' '\n');
    fi;
    echo "$OTHER_CONDA_REQUIREMENTS";
  # ---------------------------------------------------------------------------
  # The following is based on Minicoda's how-to Travis page
  # http://conda.pydata.org/docs/travis.html
  # ---------------------------------------------------------------------------
  # Download miniconda. No need to redownload if we already have the latest version cached.
  - mkdir -p $HOME/Downloads;
    travis_retry wget -c https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O "$HOME/Downloads/miniconda.sh";
  # Install miniconda to the home directory, if it isn't there already.
  - if [ ! -d "$HOME/miniconda/bin" ]; then
      if [ -d "$HOME/miniconda" ]; then rm -r "$HOME/miniconda"; fi;
      bash $HOME/Downloads/miniconda.sh -b -p "$HOME/miniconda";
    fi;
  - ls -alh "$HOME/miniconda";
  # Add conda to the path and automatically say yes to any check from conda
  - export PATH="$HOME/miniconda/bin:$PATH";
    hash -r;
    conda config --set always_yes yes --set changeps1 no
  # Remove test environment from conda, if it's still there from last time
  - conda remove -n testenv --all || echo "No 'testenv' environment to remove";
  # Update conda
  - travis_retry conda update -q conda
  # Useful for debugging any issues with conda
  - conda info -a
  - conda list
  #
  # If necessary, check which is the earliest version of numpy and scipy
  # available on conda for this version of python.
  # Because any given version of scipy is only available for a narrow range
  # of numpy versions, we constrain only scipy and not numpy to its oldest
  # possible requirement when scipy is being installed. The version of numpy
  # we end up must still satisfy the original requirement.txt setting, and
  # be from around the time of the oldest supported scipy release.
  - if [[ "$OLDEST_DEPS" == "true" ]]; then
      if [[ "$SCIPY_REQUIREMENT" != "" ]]; then
          SCIPY_VERSION="$(bash
              ./.ci/conda_min_version.sh
              "$SCIPY_REQUIREMENT" "$TRAVIS_PYTHON_VERSION")";
          if [[ "$SCIPY_VERSION" != "" ]]; then
            SCIPY_REQUIREMENT="scipy==$SCIPY_VERSION";
          fi;
      elif [[ "$NUMPY_REQUIREMENT" != "" ]]; then
          NUMPY_VERSION="$(bash
              ./.ci/conda_min_version.sh
              "$NUMPY_REQUIREMENT" "$TRAVIS_PYTHON_VERSION")";
          if [[ "$NUMPY_VERSION" != "" ]]; then
            NUMPY_REQUIREMENT="numpy==$NUMPY_VERSION";
          fi;
      fi;
    fi;
  # Create the conda environment with pip, numpy and scipy installed (if they
  # are in requirements.txt)
  - conda create -q -n testenv python=$TRAVIS_PYTHON_VERSION
      pip $NUMPY_REQUIREMENT $SCIPY_REQUIREMENT $OTHER_CONDA_REQUIREMENTS
  # If you get an error from this command which looks like this:
  #   Error: Unsatisfiable package specifications.
  #   Generating hint:
  #   [      COMPLETE      ]|###########| 100%
  #   Hint: the following packages conflict with each other:
  #     - numpy >=1.9.0
  #     - scipy ==0.12.0
  #
  # This is because you have constrained the numpy version in requirements.txt
  # to a more recent set of values (e.g. numpy>=1.9.0) than the scipy
  # constraint (e.g. scipy>=0.12.0). The OLDEST_DEPS code has
  # looked up the oldest compatible version available on conda (scipy==0.12.0)
  # but there is no numpy version for this which matches your constraint.
  #
  # You can resolve this by doing a search of the conda packages available
  #   conda search scipy
  # and changing your scipy constraint to be scipy>=x.y.z, where x.y.z is the
  # oldest version which has a matching numpy version in its buildstring.
  # To resolve the example, we look for the first scipy version which has
  # 'np19' in its buildstring, and find it is scipy version 0.14.0, so we
  # update the requirements.txt file to have 'scipy>=0.14.0' instead of
  # 'scipy>=0.12.0'.
  #
  # Activate the test environment
  - source activate testenv

###############################################################################
install:
  # Install required packages listed in requirements.txt. We install this
  # with the --upgrade flag *and* with --no-deps to make sure we have the
  # most up to date version of all *immediate* dependencies of the package
  # we are developing (whilst still compatible with the version specifier),
  # without upgrading recursively (since that would invariably involve
  # upgrading numpy and/or scipy). We then need to make sure the dependency
  # "chain" is satisfied (dependencies of dependencies are adequate) without
  # upgrading unnecessarily.
  - if [ -f requirements.txt ]; then
      cat requirements.txt;
      pip install --no-deps --upgrade -r requirements.txt;
      pip install -r requirements.txt;
    fi;
  # Also install any developmental requirements, if present.
  - if [ -f requirements-dev.txt ]; then
      cat requirements-dev.txt;
      pip install --no-deps --upgrade -r requirements-dev.txt;
      pip install -r requirements-dev.txt;
    fi;
  - if [ -f requirements-test.txt ]; then
      cat requirements-test.txt;
      pip install --no-deps --upgrade -r requirements-test.txt;
      pip install -r requirements-test.txt;
    fi;
  # ---------------------------------------------------------------------------
  # Conditionally install the packages which are needed for building docs
  - if [[ "$BUILD_DOCS" == "true" ]]; then
      pip install -r requirements-docs.txt;
    fi
  # ---------------------------------------------------------------------------
  # Now install your own package
  - python setup.py install

###############################################################################
before_script:
  # Double-check we are still in the right directory
  - pwd
  # Check what python packages we have installed
  - conda info -a
  - which python
  - python --version
  - conda env export > environment.yml && cat environment.yml
  - pip freeze
  # ---------------------------------------------------------------------------
  # Remove any cached results files from previous build, if present
  - rm -f testresults.xml;
    rm -f coverage.xml;
    rm -f .coverage;
  # ---------------------------------------------------------------------------
  # Set up folders for test results on Shippable
  - if [ "$SHIPPABLE" = "true" ]; then
      rm -fr shippable;
      mkdir -p shippable/testresults;
      mkdir -p shippable/codecoverage;
    fi;

###############################################################################
script:
  - python --version;
    if [[ "$NUMPY_REQUIREMENT" != "" ]]; then
      python -c "import numpy; print('numpy %s' % numpy.__version__)";
    fi;
    if [[ "$SCIPY_REQUIREMENT" != "" ]]; then
      python -c "import scipy; print('scipy %s' % scipy.__version__)";
    fi;
  # ---------------------------------------------------------------------------
  # Your test script goes here, e.g.
  # - python setup.py test
  - py.test --flake8 --cov=package_name --cov-report term --cov-report xml --cov-config .coveragerc --junitxml=testresults.xml
  # Build the documentation
  - if [[ "$BUILD_DOCS" == "true" ]]; then
      make -C docs html;
    fi;

###############################################################################
after_script:
  # Show where we ended up
  - pwd
  # Go back to the repository directory, just in case
  # Show what results files there are
  - cd ${REPOPATH} && ls -alh;
  # ---------------------------------------------------------------------------
  # Move results and coverage files into appropriate places
  - if [ "$SHIPPABLE" = "true" ] && [ -f testresults.xml ]; then
      mv testresults.xml shippable/testresults/;
    fi;
    if [ "$SHIPPABLE" = "true" ] && [ -f coverage.xml ]; then
      cp coverage.xml shippable/codecoverage/;
    fi;

###############################################################################
after_success:
  # Only run coveralls on Travis. When running on a public Travis-CI, the
  # repo token is automatically inferred, but to run coveralls on Shippable
  # the repo token needs to be specified in a .coveralls.yml or as an
  # environment variable COVERALLS_REPO_TOKEN. This should be kept hidden
  # from public viewing, either by encrypting the token or running on a
  # private build.
  # We ignore coveralls failures because the coveralls server is not 100%
  # reliable and we don't want the CI to report a failure just because the
  # coverage report wasn't published.
  - if [ "$TRAVIS" = "true" ] && [ "$SHIPPABLE" != "true" ]; then
      pip install coveralls;
      travis_retry coveralls || echo "Coveralls push failed";
      pip install codecov;
      travis_retry codecov || echo "Codecov push failed";
    fi;

###############################################################################
# Steps to take before archiving on Shippable (does nothing on Travis)
before_archive:
  # Have shippable archive the environment.yml artifact by putting it in
  # the REPO/shippable folder. This is available to download as a tar file for
  # each build.
  # Since this build was successful, you can share it for users to install from
  # with the command `conda env create -f environment.yml` and know they have
  # a working build.
  # If you want to save this file on Travis, you will need to turn on the
  # artifacts addon (or do something else with it). See here for details
  # https://docs.travis-ci.com/user/uploading-artifacts/
  - if [ "$SHIPPABLE" = "true" ] && [ -f environment.yml ]; then
      cp environment.yml shippable/;
    fi;

###############################################################################
# Enable archiving of artifacts on Shippable (does nothing on Travis)
archive: true
